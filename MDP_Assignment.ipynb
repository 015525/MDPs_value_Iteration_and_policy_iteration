{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "P0tapsv9q3Ib"
      },
      "outputs": [],
      "source": [
        "def draw_utility (utility):\n",
        "  for i in range(3):\n",
        "    for j in range(3):\n",
        "      print(\"{:.2f}\\t\".format(utility[i][j]), end = \"\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_policy(policy):\n",
        "  s = [[str(e) for e in row] for row in policy]\n",
        "  lens = [max(map(len, col)) for col in zip(*s)]\n",
        "  fmt = '\\t'.join('{{:{}}}'.format(x) for x in lens)\n",
        "  table = [fmt.format(*row) for row in s]\n",
        "  print('\\n'.join(table))\n"
      ],
      "metadata": {
        "id": "LGV8Zn4EFeNh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ideas\n",
        "\n",
        "\n",
        "we use value iteration and policy iteration to get the optimal policy\n",
        "\n",
        "**value iteration** starts from utality = 0 then start to update the values using bellman equation until it convergs and then stop\n",
        "\n",
        "**policy iteration** start with random utality and then use bellman equtaion to update utality until it convergs then stop"
      ],
      "metadata": {
        "id": "fkEdJtFHssty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithms"
      ],
      "metadata": {
        "id": "GEpEHzIbA8_H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1- Value Iteration\n",
        "\n",
        "\n",
        "\n",
        "1.   Go through each square on the grid\n",
        "2.   Update the utility using Bellman equation\n",
        "3.   Keep iterate and update the utility while the difference between new value and old value < x \n",
        "4.   Return the final policy \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "O32wPiP2A_Na"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summation2(action, reward, utility, tansition_dic, discount, i, j) :\n",
        "  n = i - 1 if i > 0 else  0\n",
        "  s = i + 1 if i < 2 else  2\n",
        "  e = j + 1 if j < 2 else  2\n",
        "  w = j - 1 if j > 0 else  0\n",
        "  ans = tansition_dic[action][0]*(reward[n][j] + discount * utility[n][j]) +\\\n",
        "        tansition_dic[action][1]*(reward[s][j] + discount * utility[s][j]) +\\\n",
        "        tansition_dic[action][2]*(reward[i][e] + discount * utility[i][e]) +\\\n",
        "        tansition_dic[action][3]*(reward[i][w] + discount * utility[i][w]) \n",
        "\n",
        "  return ans"
      ],
      "metadata": {
        "id": "texR4Q0ADP6_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import time\n",
        "import copy \n",
        "\n",
        "def value_iteration(r, threshold):\n",
        "  utility = [[0,0,0],[0,0,0],[0, 0, 0]]  # utility of the board starts with 0s\n",
        "\n",
        "  reward = [[r, -1, 10], [-1, -1 , -1], [-1, -1, -1]] # reward of each square in the grid\n",
        "  gama = 0.99     # discount factor \n",
        "  actions = ['n', 's', 'e', 'w']  # available actions \n",
        "  \n",
        "  tansition_dic = {               # actions probabilities \n",
        "      'n' : [0.8, 0 ,0.1, 0.1],\n",
        "      's' : [0, 0.8, 0.1, 0.1],\n",
        "      'e' : [0.1, 0.1, 0.8, 0],\n",
        "      'w' : [0.1, 0.1, 0, 0.8],\n",
        "  }\n",
        "\n",
        "  policy =[[r , '', '+10'], ['', '' ,''], ['', '', '']]  # the policy of the agent \n",
        "  \n",
        "  thres = 100\n",
        "  counter = 0\n",
        "  while True:        # for loop only for testing, will be while later\n",
        "    if(thres < threshold):            # if all Vi+1 - Vi < threshold then we reach our limit break\n",
        "      break\n",
        "    if counter == 0 :\n",
        "      utility = [[r, 0, 10], [0, 0, 0], [0, 0, 0]]\n",
        "      counter +=1\n",
        "      continue\n",
        "\n",
        "    counter +=1\n",
        "    \n",
        "    thres = 0\n",
        "    v = copy.deepcopy(utility)         # save the old  values of utility to compare it with the new vlaues\n",
        "    delta = 0\n",
        "    \n",
        "    # go through all the squresin the grid\n",
        "    for i in range(3):\n",
        "      for j in range(3):\n",
        "        if (i==0) and (j==0 or j==2) : continue\n",
        "        prev = summation2('n', reward, v, tansition_dic, gama, i, j) \n",
        "        prevAction = 'n'\n",
        "        for action in actions:      #go through all actions for each square on the grid\n",
        "          utility[i][j] = summation2(action, reward, v, tansition_dic, gama, i, j)    #get the utility of each action\n",
        "          policy[i][j] = action\n",
        "          \n",
        "          if utility[i][j] < prev :\n",
        "            utility[i][j] = prev\n",
        "            policy[i][j] = prevAction\n",
        "          else :\n",
        "            prev = utility[i][j]\n",
        "            prevAction = action\n",
        "\n",
        "    for i in range(3):\n",
        "      for j in range(3):\n",
        "        thres = max(thres, abs(utility[i][j] - v[i][j]))\n",
        "  print(\"number of iterations : \" + str(counter))\n",
        "\n",
        "  #draw_utility(utility)\n",
        "  return policy"
      ],
      "metadata": {
        "id": "5TJc100ADRR0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2- Policy Iteration \n",
        "\n",
        "1.  start with random initial policy\n",
        "2.  evaluate the policy to get the initial utility using bellman equation \n",
        "3.  update the policy according to the new utility \n",
        "4.  return the final policy "
      ],
      "metadata": {
        "id": "7E9Opb8hDa1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def get_random_pos():\n",
        "  i = random.randint(0,2)\n",
        "  j = random.randint(0,2)\n",
        "  return i, j"
      ],
      "metadata": {
        "id": "tXOVBg_s-ByH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_random_policy(r):\n",
        "  moves = ['n', 's', 'e','w']\n",
        "  policy = [[r, 'b', '+10'],['a', 'b', 'c'],['a', 'b', 'c']]\n",
        "  for i in range (0, 3):\n",
        "    for j in range (0, 3):\n",
        "      if (i==0) and (j==0 or j==2) : continue\n",
        "      policy[i][j] = moves[random.randint(0,3)]\n",
        " \n",
        "  return policy"
      ],
      "metadata": {
        "id": "8SKZYGYaFgUO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "def policy_evaluation(reward, policy, utility, discount_factor):\n",
        "  threshold = 0.001\n",
        "  converge = False\n",
        "  tansition_dic = {\n",
        "      'n' : [0.8, 0 ,0.1, 0.1],\n",
        "      's' : [0, 0.8, 0.1, 0.1],\n",
        "      'e' : [0.1, 0.1, 0.8, 0],\n",
        "      'w' : [0.1, 0.1, 0, 0.8],\n",
        "  }\n",
        "  discount = 0.99\n",
        "  policy_evaluation_counter = 0\n",
        "  while not converge:\n",
        "    policy_evaluation_counter+=1\n",
        "    delta = 0\n",
        "    prev_utility = [row[:] for row in utility]\n",
        "    for i in range(0, 3):\n",
        "      for j in range(0, 3):\n",
        "        if (i==0) and (j==0 or j==2) : continue\n",
        "        temp = utility[i][j]\n",
        "        utility[i][j] = summation2(policy[i][j], reward, prev_utility, tansition_dic, discount, i, j)\n",
        "        delta = max(delta, abs(temp - utility[i][j]))\n",
        "\n",
        "    if delta < threshold:\n",
        "      converge = True\n",
        "  return utility"
      ],
      "metadata": {
        "id": "Qz--Q7KHHcBH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_action(utility, reward, discount,  current_i, current_j) :\n",
        "  actions = ['n', 's', 'e', 'w']\n",
        "  tansition_dic = {\n",
        "      'n' : [0.8, 0 ,0.1, 0.1],\n",
        "      's' : [0, 0.8, 0.1, 0.1],\n",
        "      'e' : [0.1, 0.1, 0.8, 0],\n",
        "      'w' : [0.1, 0.1, 0, 0.8],\n",
        "  }\n",
        "  max_action = 'n'\n",
        "  max_value = summation2(max_action, reward, utility, tansition_dic, discount, current_i, current_j)\n",
        "  for action in actions :\n",
        "    value = summation2(action, reward, utility, tansition_dic, discount, current_i, current_j)\n",
        "    if value > max_value :\n",
        "      max_value = value\n",
        "      max_action = action\n",
        "\n",
        "  return max_action"
      ],
      "metadata": {
        "id": "9RFVCNE2FHmO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_improvement(policy, utility, reward, discount):\n",
        "\n",
        "  change  = False\n",
        "  for i in range(3) :\n",
        "    for j in range(3) :\n",
        "      if (i==0) and (j==0 or j==2) : continue\n",
        "      temp = policy[i][j]\n",
        "      policy[i][j] = get_max_action(utility, reward, discount, i, j)\n",
        "      if temp != policy[i][j] :\n",
        "        change = True\n",
        "  return policy , change"
      ],
      "metadata": {
        "id": "79hDoo4wHg4F"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(r):\n",
        "  policy = get_random_policy(r)\n",
        "  discount_factor = 0.99\n",
        "  utility = [[0, 0, 0], [0, 0, 0], [0, 0, 0]]\n",
        "  reward = [[r, -1, 10], [-1, -1 , -1], [-1, -1, -1]]\n",
        "  stable = False\n",
        "\n",
        "  iterations_counter = 0\n",
        "  while not stable:\n",
        "    if not iterations_counter :\n",
        "      utility = [[r, 0, 10], [0, 0, 0], [0, 0, 0]]\n",
        "      iterations_counter += 1\n",
        "      continue\n",
        "    iterations_counter += 1\n",
        "    utility = policy_evaluation(reward, policy, utility, discount_factor)\n",
        "    policy, change = policy_improvement(policy, utility, reward, discount_factor)\n",
        "    if not change:\n",
        "      stable = True\n",
        "      \n",
        "  print(\"number of iterations is : \" + str(iterations_counter))\n",
        "  #draw_utility(utility)\n",
        "  return policy "
      ],
      "metadata": {
        "id": "a_i53kk9FNQu"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# output Policies "
      ],
      "metadata": {
        "id": "abnHNsTMxrsr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### r = 100"
      ],
      "metadata": {
        "id": "MogPrStKxv5k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Value Iteration"
      ],
      "metadata": {
        "id": "P6WGEWyNx1F6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy = value_iteration(100, 0.000001)\n",
        "print()\n",
        "draw_policy(policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdzXrlpq_kVb",
        "outputId": "9996e79f-b961-4692-82f3-6a951e58f8fc"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of iterations : 33\n",
            "\n",
            "100\tw\t+10\n",
            "n  \tw\ts  \n",
            "n  \tw\tw  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Policy Itreation "
      ],
      "metadata": {
        "id": "oYaAeFNex42Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy = policy_iteration(100)\n",
        "print()\n",
        "draw_policy(policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hp-iDwapx7Uf",
        "outputId": "2c687247-6c84-4ad3-ae33-6585696f8885"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of iterations is : 4\n",
            "\n",
            "100\tw\t+10\n",
            "n  \tw\ts  \n",
            "n  \tw\tw  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**the optimal policy tends to get the highest reward, so it escape from the smalles reward r = 10 by going south and move to the left side of the grid to get the highest reward r = 100**"
      ],
      "metadata": {
        "id": "Q2n81IFq-c0u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### r = 3"
      ],
      "metadata": {
        "id": "SqmO_7r4ywHH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Valeu Iteration"
      ],
      "metadata": {
        "id": "-EOqPpeYyv5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy = value_iteration(3, 0.000001)\n",
        "print()\n",
        "draw_policy(policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gv_YeHWbj8Nn",
        "outputId": "5804f6cd-1815-4baf-996e-04a1631530e0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of iterations : 28\n",
            "\n",
            "3\te\t+10\n",
            "e\te\tn  \n",
            "e\te\tn  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Policy Iteration"
      ],
      "metadata": {
        "id": "ueVJd--Gy2rx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy = policy_iteration(3)\n",
        "print()\n",
        "draw_policy(policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGhY8VWj4X1d",
        "outputId": "9c2eb0ae-120e-493d-8862-edbc4a71ba41"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of iterations is : 4\n",
            "\n",
            "3\te\t+10\n",
            "e\te\tn  \n",
            "e\te\tn  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**the policy tends to get the highest reward so it moves to the right side with the highest reward r = 10**"
      ],
      "metadata": {
        "id": "5AZWsN7E-ThM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### r = 0"
      ],
      "metadata": {
        "id": "1ibhbPyD4opi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Value Iteration"
      ],
      "metadata": {
        "id": "Voi79eCO4od-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy = value_iteration(0, 0.000001)\n",
        "print()\n",
        "draw_policy(policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAXADbrWj8Fl",
        "outputId": "df8c1fb6-7efb-416b-9366-391c07eeb64e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of iterations : 28\n",
            "\n",
            "0\te\t+10\n",
            "e\te\tn  \n",
            "e\te\tn  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Policy Iteration"
      ],
      "metadata": {
        "id": "b_GD76_24uDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy = policy_iteration(0)\n",
        "print()\n",
        "draw_policy(policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XHU_1JS4twj",
        "outputId": "40e04a43-6de5-4ed8-9208-5ca69f7aafc8"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of iterations is : 4\n",
            "\n",
            "0\te\t+10\n",
            "e\te\tn  \n",
            "e\te\tn  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**the policy tends to get the highest reward so it moves to the right side with the highest reward r = 10**"
      ],
      "metadata": {
        "id": "fPcGVecW-P0Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### r = -3"
      ],
      "metadata": {
        "id": "mkaDW1R-45KB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Value Iteration"
      ],
      "metadata": {
        "id": "Ay55GqwU449f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy = value_iteration(-3, 0.000001)\n",
        "print()\n",
        "draw_policy(policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G02uV1hkj77v",
        "outputId": "a258e653-8aef-46ae-f6a6-7a2adb741d71"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of iterations : 28\n",
            "\n",
            "-3\te\t+10\n",
            "e \te\tn  \n",
            "e \te\tn  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Policy Iteration"
      ],
      "metadata": {
        "id": "Bs7xNuqF4_Pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy = policy_iteration(-3)\n",
        "print()\n",
        "draw_policy(policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXJCQgXM4-y4",
        "outputId": "358e4345-62b3-40cc-9322-5b5b44bccf46"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of iterations is : 3\n",
            "\n",
            "-3\te\t+10\n",
            "e \te\tn  \n",
            "e \te\tn  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**the policy tends to get the highest reward so it moves to the right side with the highest reward r = 10**"
      ],
      "metadata": {
        "id": "2jcNJ3GQ61YP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### r = -300"
      ],
      "metadata": {
        "id": "PoYnsJtqzTig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Value Iteration"
      ],
      "metadata": {
        "id": "DULKrPK7zTGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy = value_iteration(-300, 0.000001)\n",
        "print()\n",
        "draw_policy(policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHjiGlPoj7vt",
        "outputId": "2aa01551-4629-4dc9-c4dc-9c6474452ff1"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of iterations : 30\n",
            "\n",
            "-300\te\t+10\n",
            "s   \te\tn  \n",
            "e   \te\tn  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Policy Iteartion "
      ],
      "metadata": {
        "id": "U65o2GKxzZLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy = policy_iteration(-300)\n",
        "print()\n",
        "draw_policy(policy)"
      ],
      "metadata": {
        "id": "gAxKjVEdkNoj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fea851f5-32d0-4356-fada-ee9abc2b3989"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of iterations is : 4\n",
            "\n",
            "-300\te\t+10\n",
            "s   \te\tn  \n",
            "e   \te\tn  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**policy tends to excape form the small value and move to the highest value, So It escapes from this exit square with r = -300 as it makes much loss by moving south and then direct to the other exit with the highest reward**"
      ],
      "metadata": {
        "id": "5qxv7p-f55W7"
      }
    }
  ]
}